# ğŸ§  CrackDSA â€“ Your Local AI Copilot for Proctored DSA Rounds (For Fun ğŸ˜‰)

**ClearDSA** is a macOS-native desktop chat assistant built with Flutter. It connects to local Ollama LLMs like `llama3`, `mistral`, and `codellama` to provide discreet, offline-friendly help for Data Structures & Algorithms (DSA) challengesâ€”even during proctored coding rounds (strictly for "educational purposes" of course).

> ğŸ¤– Because sometimes even geniuses need a quiet nudge.

---

## ğŸš€ Features

- **Private & Local**  
  Connects to `http://localhost:11434` â€” no internet, no data sharing, just pure local AI inference.

- **Minimal Chat Interface**  
  Clean, distraction-free UI tailored for high-pressure DSA environments.

- **Model Switching On The Fly**  
  Toggle between powerful LLMs (e.g., `llama3`, `codellama`, `mistral`) based on your coding context.

- **Instant Connection Status**  
  Know exactly when your local Ollama server is ready.

- **Formatted Code Output**  
  Beautified display of code blocks for quick copy-paste.

- **Chrome Tab Or Window Overlay**  
  Open the chat assistant on top of any coding window without getting caught

---

## âš™ï¸ Setup

### 1. Requirements

- macOS (tested on Ventura+)
- [Flutter](https://flutter.dev)
- [Ollama](https://ollama.com) (for local LLMs)

### 2. Install Ollama & Models

```bash
brew install ollama
ollama serve
ollama run llama3   # or mistral, codellama, etc.
````

### 3. Clone & Run

```bash
git clone https://github.com/your-username/ClearDSA
cd ClearDSA
flutter pub get
flutter run -d macos
```

---

## ğŸ’¡ Ideal Use Case

* Running in split-screen beside your browser-based proctored coding test (ğŸ˜‰)
* Typing quick DSA queries like:

  * â€œWrite a recursive solution to generate all balanced parenthesesâ€
  * â€œOptimize this O(nÂ²) approach using a hashmapâ€
  * â€œWhatâ€™s the difference between BFS and DFS?â€

> âš ï¸ This app does **not** bypass any monitoring or screen sharing tools. Use responsibly.

---

## ğŸ§  Supported Models

* `llama3`
* `mistral`
* `codellama`
* `gemma`
* `llava` (vision + text)

> Add more via `ollama run <model-name>`

---

## ğŸ› ï¸ File Structure

```bash
lib/
â”œâ”€â”€ main.dart               # App entry point
â”œâ”€â”€ ollama_chat.dart        # Chat UI + logic
â”œâ”€â”€ model_selector.dart     # Dropdown to switch models
â”œâ”€â”€ connection_status.dart  # Ollama status indicator
â””â”€â”€ code_block_widget.dart  # Styled output for AI code responses
```

---

## ğŸ“¸ Preview

> *Screenshot coming soon: clean chat UI, dark mode, real-time responses*

---

## ğŸ“œ License

MIT License â€” do whatever you want, but don't blame me if you get caught using it in a real interview ğŸ˜„

---

## â­ï¸ Fun Disclaimer

This tool is **for educational and entertainment purposes only**.
Please respect exam integrity policies. But hey... what you do on localhost stays on localhost.

---
